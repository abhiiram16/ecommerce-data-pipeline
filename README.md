# ğŸª E-Commerce Data Pipeline

> A production-grade end-to-end data pipeline demonstrating data engineering best practices with synthetic e-commerce data.

[![Python](https://img.shields.io/badge/Python-3.11-blue.svg)](https://www.python.org/)
[![PostgreSQL](https://img.shields.io/badge/PostgreSQL-15-blue.svg)](https://www.postgresql.org/)
[![Apache Airflow](https://img.shields.io/badge/Apache%20Airflow-2.7.3-red.svg)](https://airflow.apache.org/)
[![Docker](https://img.shields.io/badge/Docker-24.0-blue.svg)](https://www.docker.com/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

---

## ğŸ“‹ Table of Contents

- [Overview](#overview)
- [Architecture](#architecture)
- [Technologies Used](#technologies-used)
- [Key Features](#key-features)
- [Project Structure](#project-structure)
- [Setup Instructions](#setup-instructions)
- [Usage Guide](#usage-guide)
- [Data Quality](#data-quality)
- [Orchestration](#orchestration)
- [Cloud Deployment](#cloud-deployment)
- [Skills Demonstrated](#skills-demonstrated)
- [Results & Metrics](#results--metrics)
- [Future Enhancements](#future-enhancements)
- [Contact](#contact)

---

## ğŸ¯ Overview

This project implements a complete **data engineering pipeline** that processes synthetic e-commerce data through all stages of the data lifecycle: generation, ingestion, transformation, quality validation, and orchestration.

**Built to demonstrate production-ready data engineering skills for interviews and portfolio showcase.**

### Business Context

Simulates a real-world e-commerce platform analyzing:
- **Customer behavior** (purchases, lifetime value, RFM segmentation)
- **Product performance** (revenue, margin, bestsellers)
- **Sales trends** (daily/monthly patterns, seasonality)

### What Makes This Special

âœ… **End-to-end pipeline** - Not just a script, a complete system  
âœ… **Production patterns** - Error handling, logging, monitoring  
âœ… **Scalable architecture** - Designed for growth (60K â†’ 6M records)  
âœ… **Automated workflows** - Apache Airflow orchestration  
âœ… **Data quality focus** - 4-dimension validation framework  
âœ… **Cloud-ready** - AWS deployment strategy documented  

---

## ğŸ—ï¸ Architecture

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ DATA PIPELINE ARCHITECTURE â”‚

[Data Generation] [Ingestion] [Storage]
â†“ â†“
â†“ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â”€â”€â”€â” â”‚ Faker â”‚ â”€â”€â”€â”€CSVâ”€â”€â†’ â”‚ Python â”‚ â”€â”€â”€SQLâ”€â”€â†’â”‚ PostgreS
L â”‚ â”‚ Library â”‚ â”‚ Pandas â”‚ â”‚ Databas
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â”€â”€â”€â”˜ 60K records Batch Load 15 tables
v
ews


â†“

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚Transforma
ionâ”‚ â”‚
QL Queries â”‚ â””â”€â”€â”€â”€
â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
â†“ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ â”‚ â†“
â†“ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”

### Pipeline Stages

1. **Data Generation** - Faker library creates realistic synthetic data
2. **Ingestion** - Python scripts load CSV to PostgreSQL
3. **Transformation** - SQL creates aggregates and analytical views
4. **Quality Validation** - 4-dimension checks (completeness, validity, consistency, uniqueness)
5. **Orchestration** - Apache Airflow schedules and monitors workflows

---

## ğŸ› ï¸ Technologies Used

| Category | Technology | Purpose |
|----------|------------|---------|
| **Language** | Python 3.11 | Data processing, scripting |
| **Database** | PostgreSQL 15 | Data warehouse, storage |
| **Orchestration** | Apache Airflow 2.7.3 | Workflow automation |
| **Containerization** | Docker & Docker Compose | Environment isolation |
| **Data Processing** | Pandas 2.2.0 | ETL transformations |
| **Data Generation** | Faker 22.6.0 | Synthetic data creation |
| **Version Control** | Git & GitHub | Code management |
| **Documentation** | Markdown | Project docs |

---

## âœ¨ Key Features

### 1. Synthetic Data Generation
- **60,495 total records** across 3 entities
- **10,000 customers** with realistic demographics
- **495 products** across multiple categories
- **50,000 orders** with temporal patterns

### 2. Automated ETL Pipeline
- Batch processing with error handling
- Transaction management for data integrity
- Logging for observability
- Retry logic for resilience

### 3. Data Transformation
- **4 aggregate tables** (customer summary, product summary, daily sales, monthly sales)
- **5 analytical views** for reporting
- Complex SQL with CTEs, window functions, joins

### 4. Data Quality Framework
- **4-dimension validation**:
  - Completeness: 100%
  - Validity: 100%
  - Consistency: 99%
  - Uniqueness: 100%
- **Anomaly detection** using Z-score statistical method
- **HTML quality dashboard** with visual reports

### 5. Workflow Orchestration
- **3 Apache Airflow DAGs**:
  - `daily_quality_check` - Parallel quality validation
  - `refresh_aggregations` - Fan-out-fan-in pattern
  - `weekly_full_pipeline` - Sequential 6-stage ETL
- Scheduled execution (cron expressions)
- Error handling & automatic retries
- Monitoring UI with task-level visibility

### 6. Cloud Architecture (Documented)
- AWS deployment strategy
- Cost analysis ($18.67/month)
- Scalability plan (100x growth path)
- Security best practices
- Migration roadmap

---

## ğŸ“ Project Structure

ecommerce-data-pipeline/
â”œâ”€â”€ data/
â”‚ â”œâ”€â”€ raw/ # Generated CSV files
â”‚ â””â”€â”€ processed/ # Quality reports
â”œâ”€â”€ src/
â”‚ â”œâ”€â”€ generation/ # Data generators
â”‚ â”‚ â”œâ”€â”€ generate_customers.py
â”‚ â”‚ â”œâ”€â”€ generate_products.py
â”‚ â”‚ â””â”€â”€ generate_orders.py
â”‚ â”œâ”€â”€ ingestion/ # ETL scripts
â”‚ â”‚ â”œâ”€â”€ load_customers.py
â”‚ â”‚ â”œâ”€â”€ load_products.py
â”‚ â”‚ â””â”€â”€ load_orders.py
â”‚ â”œâ”€â”€ processing/ # Transformation scripts
â”‚ â”‚ â””â”€â”€ refresh_aggregations.py
â”‚ â””â”€â”€ quality/ # Quality validation
â”‚ â”œâ”€â”€ data_quality_checks.py
â”‚ â”œâ”€â”€ detect_anomalies.py
â”‚ â””â”€â”€ generate_quality_report.py
â”œâ”€â”€ sql/
â”‚ â”œâ”€â”€ schema.sql # Table definitions
â”‚ â”œâ”€â”€ aggregations.sql # Summary tables
â”‚ â””â”€â”€ views.sql # Analytical views
â”œâ”€â”€ airflow/
â”‚ â””â”€â”€ dags/ # Airflow DAG definitions
â”‚ â”œâ”€â”€ daily_quality_check.py
â”‚ â”œâ”€â”€ refresh_aggregations.py
â”‚ â””â”€â”€ weekly_full_pipeline.py
â”œâ”€â”€ docs/ # Phase documentation
â”œâ”€â”€ docker-compose.yml # PostgreSQL container
â”œâ”€â”€ docker-compose-airflow.yml # Airflow containers
â”œâ”€â”€ requirements.txt # Python dependencies
â””â”€â”€ README.md # This file

---

## ğŸš€ Setup Instructions

### Prerequisites

- **Python 3.11+**
- **Docker Desktop** (for PostgreSQL & Airflow)
- **Git**
- **VS Code** (recommended)

### Installation Steps

**1. Clone the Repository**

git clone https://github.com/abhiiram16/ecommerce-data-pipeline.git
cd ecommerce-data-pipeline


**2. Create Virtual Environment**

python -m venv venv
source venv/bin/activate # On Windows: venv\Scripts\activate


**3. Install Dependencies**

pip install -r requirements.txt

**4. Start PostgreSQL Database**

docker-compose up -d


**5. Initialize Database Schema**

psql -h localhost -U dataeng -d ecommerce_db < sql/schema.sql
psql -h localhost -U dataeng -d ecommerce_db < sql/aggregations.sql
psql -h localhost -U dataeng -d ecommerce_db < sql/views.sql


*Password: `pipeline123`*

**6. Generate Synthetic Data**

python src/generation/generate_customers.py
python src/generation/generate_products.py
python src/generation/generate_orders.py


**7. Load Data to Database**

python src/ingestion/load_customers.py
python src/ingestion/load_products.py
python src/ingestion/load_orders.py

**8. Start Apache Airflow (Optional)**

docker-compose -f docker-compose-airflow.yml up -d


Access Airflow UI: `http://localhost:8080` (admin/admin)

---

## ğŸ“– Usage Guide

### Run Complete Pipeline

Generate data
python src/generation/generate_customers.py
python src/generation/generate_products.py
python src/generation/generate_orders.py

Ingest to database
python src/ingestion/load_customers.py
python src/ingestion/load_products.py
python src/ingestion/load_orders.py

Refresh aggregations
python src/processing/refresh_aggregations.py

Run quality checks
python src/quality/data_quality_checks.py

Detect anomalies
python src/quality/detect_anomalies.py

Generate HTML report
python src/quality/generate_quality_report.py

### Access Quality Dashboard

Open: `data/processed/data_quality_report.html` in browser

### Query the Data

psql -h localhost -U dataeng -d ecommerce_db


Example queries:

-- Top 10 customers by lifetime value
SELECT * FROM customer_summary
ORDER BY total_spent DESC
LIMIT 10;

-- Monthly sales trend
SELECT * FROM monthly_sales_summary
ORDER BY month;

-- Product performance
SELECT * FROM product_summary
ORDER BY total_revenue DESC
LIMIT 10;


---

## âœ… Data Quality

### Validation Dimensions

| Dimension | Score | Description |
|-----------|-------|-------------|
| **Completeness** | 100% | No null values in critical fields |
| **Validity** | 100% | All values within expected ranges |
| **Consistency** | 99% | Referential integrity maintained |
| **Uniqueness** | 100% | No duplicate primary keys |

### Overall Score: 95% (Grade A)

### Anomaly Detection

- Z-score statistical method
- Identifies outliers in order amounts
- Flags suspicious patterns
- Visual distribution charts

---

## âš™ï¸ Orchestration

### Apache Airflow DAGs

#### 1. Daily Quality Check
- **Schedule:** Every day at 2 AM IST
- **Tasks:** 5 (1 schema + 3 parallel validations + 1 scoring)
- **Pattern:** Fan-out (parallel execution)
- **Duration:** ~7 minutes

#### 2. Refresh Aggregations
- **Schedule:** Every day at 3 AM IST
- **Tasks:** 5 (4 parallel refreshes + 1 verification)
- **Pattern:** Fan-out-fan-in
- **Duration:** ~2 seconds

#### 3. Weekly Full Pipeline
- **Schedule:** Every Sunday at 4 AM IST
- **Tasks:** 6 (sequential stages)
- **Pattern:** Linear pipeline
- **Duration:** ~5 seconds

### Monitoring

Access Airflow UI at `http://localhost:8080` for:
- Real-time task status
- Execution logs
- Task duration metrics
- Failure alerts

---

## â˜ï¸ Cloud Deployment

AWS deployment architecture documented in `docs/cloud_deployment_strategy.md`

**Highlights:**
- **Cost:** $18.67/month (RDS + S3 + Lambda)
- **Scalability:** 100x growth path (60K â†’ 6M records)
- **Services:** RDS PostgreSQL, S3 Data Lake, Lambda ETL, CloudWatch Monitoring
- **Migration:** 4-day deployment plan
- **Security:** VPC isolation, encryption at rest/transit, IAM roles

---

## ğŸ“ Skills Demonstrated

### Technical Skills
âœ… **Python** - Pandas, Faker, scripting, error handling  
âœ… **SQL** - PostgreSQL, complex queries, CTEs, window functions  
âœ… **Apache Airflow** - DAG design, scheduling, orchestration patterns  
âœ… **Docker** - Containerization, Docker Compose  
âœ… **ETL/ELT** - Data pipelines, transformation logic  
âœ… **Data Quality** - Validation frameworks, anomaly detection  
âœ… **Version Control** - Git, GitHub, meaningful commits  

### Engineering Practices
âœ… **Modular design** - Separation of concerns  
âœ… **Error handling** - Try-except, logging  
âœ… **Documentation** - README, inline comments, phase notes  
âœ… **Testing** - Data validation, quality checks  
âœ… **Scalability thinking** - Cloud architecture, growth planning  

---

## ğŸ“Š Results & Metrics

### Data Processed
- **60,495 total records** across 3 entities
- **15 database objects** (tables + views)
- **4 aggregate tables** with business metrics
- **100% data quality** maintained

### Performance
- **Ingestion:** 60K records in ~30 seconds
- **Aggregation:** 4 summaries in ~15 seconds
- **Quality checks:** 4 dimensions in ~10 seconds
- **End-to-end pipeline:** <2 minutes total

### Automation
- **100% automated** workflow execution
- **3 Airflow DAGs** running on schedule
- **0 manual interventions** required
- **Real-time monitoring** enabled

---

## ğŸš€ Future Enhancements

### Phase 2 Features
- [ ] Real-time streaming with Apache Kafka
- [ ] Machine learning (customer churn prediction)
- [ ] Tableau/Power BI dashboards
- [ ] dbt for transformation layer
- [ ] Great Expectations for data testing
- [ ] CI/CD pipeline with GitHub Actions
- [ ] Unit tests with pytest
- [ ] API layer (FastAPI)

### Scalability Improvements
- [ ] Partitioning large tables by date
- [ ] Implement data lake (Parquet files)
- [ ] Add caching layer (Redis)
- [ ] Multi-region deployment
- [ ] Kubernetes for Airflow

---

## ğŸ“ Contact

**Abhiiram**  
ğŸ“§ Email: abhiramashwika@gmail.com  
ğŸ’¼ LinkedIn: [linkedin.com/in/abhiiram](https://www.linkedin.com/in/abhiiram)  
ğŸ™ GitHub: [github.com/abhiiram16](https://github.com/abhiiram16)

---

## ğŸ“„ License

This project is licensed under the MIT License - see the LICENSE file for details.

---

## ğŸ™ Acknowledgments

- Data generated using [Faker](https://faker.readthedocs.io/)
- Orchestration powered by [Apache Airflow](https://airflow.apache.org/)
- Database: [PostgreSQL](https://www.postgresql.org/)

---

## â­ Star This Project

If you found this project helpful for learning data engineering, please give it a star! â­

---

**Built with â¤ï¸ for learning data engineering and showcasing production-ready skills.**
